# ğŸ“Œ Anvin Chatbot Project

### ğŸš€ Overview
An AI-powered chatbot built using **Next.js (Frontend)** and **FastAPI (Backend)**, leveraging **Ollama** for LLM-based responses. This project allows real-time conversations with AI via a simple and interactive UI.

---

## ğŸ“‚ Project Structure
```
/anvin-project
â”‚â”€â”€ frontend/          # Next.js frontend (React + TypeScript)
â”‚â”€â”€ backend/           # FastAPI server (Python)
â”‚   â”œâ”€â”€ server.py      # FastAPI application connecting to Ollama
â”‚   â”œâ”€â”€ venv/          # Python virtual environment
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚â”€â”€ README.md          # Documentation
â”‚â”€â”€ .env.local         # Environment variables for API
```

---

## ğŸ”§ Setup & Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/anvin-chatbot.git
cd anvin-chatbot
```

### 2ï¸âƒ£ Backend Setup (FastAPI)
#### **Create a Virtual Environment & Install Dependencies**
```bash
cd backend
python3 -m venv Aviant
source Aviant/bin/activate  # For Linux/macOS
Aviant\Scripts\activate     # For Windows
pip install -r requirements.txt
```

#### **Start the Ollama Server**
```bash
ollama serve
ollama pull llama3.2  # Ensure the model is available
```

#### **Run FastAPI Server**
```bash
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

âœ… FastAPI should now be running at **http://localhost:8000/docs**.

---

### 3ï¸âƒ£ Frontend Setup (Next.js)
```bash
cd frontend
npm install  # Install dependencies
npm run dev  # Start Next.js frontend
```

âœ… Frontend should now be accessible at **http://localhost:3000**.

---

## ğŸ”— API Endpoint
| Endpoint  | Method | Description |
|-----------|--------|-------------|
| `/chat`   | POST   | Sends user input to Ollama and returns AI-generated response |

---

## ğŸ“œ Environment Variables
Create a `.env.local` file inside `frontend/` with:
```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

---

## ğŸ›  Tech Stack
- **Frontend**: Next.js (React, TailwindCSS, TypeScript)
- **Backend**: FastAPI (Python, Pydantic, HTTPX)
- **AI Engine**: Ollama (LLM integration)

---

## ğŸ“Œ To-Do & Future Improvements
- âœ… Improve UI with chat bubbles and animations
- âœ… Add persistent chat history storage
- ğŸ”² Expand AI models (support for different LLMs)
- ğŸ”² Deploy on cloud platforms (AWS/GCP/Vercel)

---

## ğŸ¤ Contributing
1. **Fork** the repo
2. **Create a new branch** (`feature-xyz`)
3. **Commit changes**
4. **Push to your fork & create a PR**

---

## ğŸ“„ License
This project is licensed under the **MIT License**.

---

ğŸ’¡ **Created by Anvin** | ğŸš€ Happy Coding! ğŸ¯

